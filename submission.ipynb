{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    seed = 42\n",
    "    max_len = 314\n",
    "    train_batch = 16\n",
    "    val_batch = 32\n",
    "    epochs = 10\n",
    "    n_tokens = 20\n",
    "    learning_rate = 1e-5\n",
    "    n_splits = 8\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./tokenizer/tokenizer_config.json',\n",
       " './tokenizer/special_tokens_map.json',\n",
       " './tokenizer/vocab.txt',\n",
       " './tokenizer/added_tokens.json',\n",
       " './tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=1)\n",
    "model.to(Config.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.save_pretrained('./tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, sentences, targets, tokenizer):\n",
    "        self.sentences = sentences\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_tokens = Config.n_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx].strip()\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=Config.max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            # return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        encoding['input_ids'] = torch.cat(\n",
    "            (\n",
    "                torch.full((1,self.n_tokens), 500).resize(20), \n",
    "                torch.tensor(encoding['input_ids'], dtype=torch.long)\n",
    "            )\n",
    "        )\n",
    "        encoding['attention_mask'] = torch.cat(\n",
    "            (\n",
    "                torch.full((1,self.n_tokens), 1).resize(20), \n",
    "                torch.tensor(encoding['attention_mask'], dtype=torch.long)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'],\n",
    "            'attention_mask': encoding['attention_mask'],\n",
    "            'targets': torch.tensor(target, dtype=torch.float)\n",
    "        }\n",
    "        \n",
    "        # return {\n",
    "        #     'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long).flatten(),\n",
    "        #     'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long).flatten(),\n",
    "        #     'targets': torch.tensor(target, dtype=torch.float)\n",
    "        # }\n",
    "\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, ids):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ids = ids\n",
    "        self.n_tokens = Config.n_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx].strip()\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=Config.max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        encoding['input_ids'] = torch.cat(\n",
    "            (\n",
    "                torch.full((1,self.n_tokens), 500).resize(20), \n",
    "                torch.tensor(encoding['input_ids'], dtype=torch.long)\n",
    "            )\n",
    "        )\n",
    "        encoding['attention_mask'] = torch.cat(\n",
    "            (\n",
    "                torch.full((1,self.n_tokens), 1).resize(20), \n",
    "                torch.tensor(encoding['attention_mask'], dtype=torch.long)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'],\n",
    "            'attention_mask': encoding['attention_mask'],\n",
    "            'ids': self.ids[idx]\n",
    "        \n",
    "        }\n",
    "\n",
    "        # return {\n",
    "        #     'input_ids': encoding['input_ids'].flatten(),\n",
    "        #     'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        #     'ids': self.ids[idx]\n",
    "        # }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(data, num_splits):\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    data[\"kfold\"] = -1\n",
    "    \n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    num_bins = int(np.floor(1 + np.log2(len(data))))\n",
    "    \n",
    "    data.loc[:, \"bins\"] = pd.cut(\n",
    "        data[\"target\"], bins=num_bins, labels=False\n",
    "    )\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=num_splits)\n",
    "\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "    \n",
    "    data = data.drop(\"bins\", axis=1)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/home/alexuvarovskyi/course_competition/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id url_legal license  \\\n",
       "0  c12129c31       NaN     NaN   \n",
       "1  85aa80a4c       NaN     NaN   \n",
       "2  b69ac6792       NaN     NaN   \n",
       "3  dd1000b26       NaN     NaN   \n",
       "4  37c1b32fb       NaN     NaN   \n",
       "\n",
       "                                             excerpt    target  standard_error  \n",
       "0  When the young people returned to the ballroom... -0.340259        0.464009  \n",
       "1  All through dinner time, Mrs. Fayre was somewh... -0.315372        0.480805  \n",
       "2  As Roger had predicted, the snow departed as q... -0.580118        0.476676  \n",
       "3  And outside before the palace a great garden w... -1.054013        0.450007  \n",
       "4  Once upon a time there were Three Bears who li...  0.247197        0.510845  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/alexuvarovskyi/course_competition/data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c85db4cc7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Another striking difference between monkeys an...</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>0.481514</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c9eea4fc6</td>\n",
       "      <td>https://simple.wikipedia.org/wiki/Patriotism</td>\n",
       "      <td>CC BY-SA 3.0 and GFDL</td>\n",
       "      <td>Patriotism means loyalty of person to his/her ...</td>\n",
       "      <td>-0.641552</td>\n",
       "      <td>0.467047</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20758ef4a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The deepest and most regularly worked was the ...</td>\n",
       "      <td>-2.217568</td>\n",
       "      <td>0.481423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a3f2f2a4b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The class had assembled again in Professor Gra...</td>\n",
       "      <td>-0.649770</td>\n",
       "      <td>0.485501</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>695bb0ad0</td>\n",
       "      <td>https://simple.wikipedia.org/wiki/Radar</td>\n",
       "      <td>CC BY-SA 3.0 and GFDL</td>\n",
       "      <td>Radar is a machine that uses radio waves for e...</td>\n",
       "      <td>-1.572898</td>\n",
       "      <td>0.442499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                     url_legal  \\\n",
       "0  c85db4cc7                                           NaN   \n",
       "1  c9eea4fc6  https://simple.wikipedia.org/wiki/Patriotism   \n",
       "2  20758ef4a                                           NaN   \n",
       "3  a3f2f2a4b                                           NaN   \n",
       "4  695bb0ad0       https://simple.wikipedia.org/wiki/Radar   \n",
       "\n",
       "                 license                                            excerpt  \\\n",
       "0                    NaN  Another striking difference between monkeys an...   \n",
       "1  CC BY-SA 3.0 and GFDL  Patriotism means loyalty of person to his/her ...   \n",
       "2                    NaN  The deepest and most regularly worked was the ...   \n",
       "3                    NaN  The class had assembled again in Professor Gra...   \n",
       "4  CC BY-SA 3.0 and GFDL  Radar is a machine that uses radio waves for e...   \n",
       "\n",
       "     target  standard_error  kfold  \n",
       "0  0.004187        0.481514      0  \n",
       "1 -0.641552        0.467047      0  \n",
       "2 -2.217568        0.481423      0  \n",
       "3 -0.649770        0.485501      0  \n",
       "4 -1.572898        0.442499      0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_folds(df, Config.n_splits)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fold = 0\n",
    "df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "train_dataset = TrainDataset(\n",
    "    sentences=df_train.excerpt.values,\n",
    "    targets=df_train.target.values,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=Config.train_batch,\n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_dataset = TrainDataset(\n",
    "    sentences=df_valid.excerpt.values,\n",
    "    targets=df_valid.target.values,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=Config.val_batch,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2479, 355)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=Config.learning_rate)\n",
    "lr_scheduler  = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=len(train_loader)*Config.epochs\n",
    ")\n",
    "\n",
    "# def loss_fn(output,target):\n",
    "#     return torch.sqrt(nn.MSELoss()(output,target))\n",
    "\n",
    "def loss_fn(output,target):\n",
    "    return nn.MSELoss()(output,target)\n",
    "scaler = amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scaler):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for data in tqdm(data_loader, total=len(data_loader)):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        targets = data['targets'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with amp.autocast():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)#, labels=targets)\n",
    "            loss = torch.sqrt(loss_fn(outputs.logits.view(-1), targets))\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader, total=len(data_loader)):\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            targets = data['targets'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n",
    "            loss = torch.sqrt(loss_fn(outputs.logits.view(-1), targets))\n",
    "\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer, device, scaler, epochs):\n",
    "    model.to(device)\n",
    "    history = defaultdict(list)\n",
    "    best_loss = np.inf\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        print('-' * 10)\n",
    "        train_loss = train_epoch(model, train_loader, loss_fn, optimizer, device, scaler)\n",
    "        print(f'Train loss {train_loss}')\n",
    "        val_loss = eval_model(model, val_loader, loss_fn, device)\n",
    "        print(f'Val loss {val_loss}')\n",
    "        print()\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        if val_loss < best_loss:\n",
    "            torch.save(model.state_dict(), f'best_model_{fold}.bin')\n",
    "            best_loss = val_loss\n",
    "            print('Model saved', f\"model loss {best_loss}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d014ec29e174496eba70478d496bb69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa08171ab604fff9441022705fa6251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.7206798234293538\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea56ed3109d94b34aaa55986472f78f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.6901050060987473\n",
      "\n",
      "Model saved model loss 0.6901050060987473\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d962f630bb0240d3b34d0e4f0019a731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.5453683495521545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eedf2da1a22485ab3ea0959baa79e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.7303712169329325\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533632429ed247c99e63e9947fe13ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.46970849460171116\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ade95111c5476b85b3d58ac8bfffe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.7213120460510254\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66a2f6abf3f40a1b0bd2868e0444eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.40655503359533124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79de798fc72e49ac92ef6e02714e189c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.7504084606965383\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0e7ea20cc545068c10f2858f10ee0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.3647599451003536\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c817f6acd9ad4848a34eb444137e0a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.6622582376003265\n",
      "\n",
      "Model saved model loss 0.6622582376003265\n",
      "Epoch 6/10\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8378f3656bdd4c65b153159a1266d047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.31514873033569707\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7dc7a2538246d782c580e5cab4065d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.7429147611061732\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966766247b2c46b78770eeb49bd372b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.291682082030081\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec801cfd597f43d5bd8de2af474d3d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.7248704880475998\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b2f11378504ac5b39a09a66a2b9f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.257558881371252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39de9b38ece148a99a89ec0b68a482f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.7508323987325033\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898cbdf0d7284753afe012c759a06ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.24510130863035878\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5876d3197b3b4e219e602d06915e3b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.7123468369245529\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d2c02406244d71848072de953020ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.23323919128987097\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98155bad8ee24e90af511d1ed988b809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.6956585496664047\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'train_loss': [0.7206798234293538,\n",
       "              0.5453683495521545,\n",
       "              0.46970849460171116,\n",
       "              0.40655503359533124,\n",
       "              0.3647599451003536,\n",
       "              0.31514873033569707,\n",
       "              0.291682082030081,\n",
       "              0.257558881371252,\n",
       "              0.24510130863035878,\n",
       "              0.23323919128987097],\n",
       "             'val_loss': [0.6901050060987473,\n",
       "              0.7303712169329325,\n",
       "              0.7213120460510254,\n",
       "              0.7504084606965383,\n",
       "              0.6622582376003265,\n",
       "              0.7429147611061732,\n",
       "              0.7248704880475998,\n",
       "              0.7508323987325033,\n",
       "              0.7123468369245529,\n",
       "              0.6956585496664047]})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    device=Config.device,\n",
    "    epochs=Config.epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=1)\n",
    "eval_model.load_state_dict(torch.load(f'best_model_{fold}.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, model):\n",
    "    dataset = EvalDataset(\n",
    "        sentences=df.excerpt.values,\n",
    "        tokenizer=tokenizer,\n",
    "        ids=df.id.values\n",
    "    )\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=Config.val_batch,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    model.to(Config.device)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader, total=len(data_loader)):\n",
    "            input_ids = data['input_ids'].to(Config.device)\n",
    "            attention_mask = data['attention_mask'].to(Config.device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds.extend(outputs.logits.view(-1).cpu().numpy())\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f3908dfd5a4c4cadfa3a18f3dd2b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "predicts = predict(df_test, eval_model)\n",
    "df_test['target'] = predicts\n",
    "df_test[['id', 'target']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
